
<!DOCTYPE html>
<html>
<style>

p {
  font-size: 15px;
}

</style>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Exploring Unsupervised</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1.">


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <link rel="icon" type="image/png" href="img/seal_icon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110862391-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110862391-1');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main" >
        <div class="row">
            <h2 class="col-md-12 text-center">
                Beyond Supervised vs. Unsupervised: Representative Benchmarking and Analysis of Image Representation Learning</br> 
                <small>
                    CVPR 2022 
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://mgwillia.github.io">
                          Matt Gwilliam
                        </a>
                    </li>
                    <li>
                        <a href="http://www.cs.umd.edu/~abhinav/">
                          Abhinav Shrivastava
                        </a>
                    </li>
                </ul>
                University of Maryland, College Park
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                               <h4><strong>Paper (Coming Soon)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/mgwillia/unsupervised-analysis">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    By leveraging contrastive learning, clustering, and other pretext tasks, unsupervised methods for learning image representations have reached impressive results on standard benchmarks. The result has been a crowded field - many methods with substantially different implementations yield results that seem nearly identical on popular benchmarks, such as linear evaluation on ImageNet. However, a single result does not tell the whole story. In this paper, we compare methods using performance-based benchmarks such as linear evaluation, nearest neighbor classification, and clustering for several different datasets, demonstrating the lack of a clear frontrunner within the current state-of-the-art. In contrast to prior work that performs only supervised vs. unsupervised comparison, we compare several different unsupervised methods against each other. To enrich this comparison, we analyze embeddings with measurements such as uniformity, tolerance, and centered kernel alignment (CKA), and propose two new metrics of our own: nearest neighbor graph similarity and linear prediction overlap. We reveal through our analysis that in isolation, single popular methods should not be treated as though they represent the field as a whole, and that future work ought to consider how to leverage the complimentary nature of these methods. We also leverage CKA to provide a framework to robustly quantify augmentation invariance, and provide a reminder that certain types of invariance will be undesirable for downstream tasks.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <center><image src="imgs/teaser_star.png" height="240px"></image> </center> <br>
                <p class="text-justify">
                    We examine a few popular assumptions about unsupervised image representation learning. We add our voices to recent evidence that there is no clear "best" method, per current methods. We show, such as in the figure above, that unsupervised methods learn representations that are quite distinct from each other (and the analysis community should not treat them as though they are interchangeable), even though some have similar objectives. We also take a closer look at augmentation invariance, providing evidence where others have only speculated on the interactions between color jitter and unsupervised training objectives.  
                </p>

                <h3>
                    No clear "best" method
                </h3>
                <center><image src="imgs/all_frozen_results.png" height="240px"></image> </center> <br>
                <p class="text-justify">
                    No single method achieves the highest accuracy on every dataset for these ResNet-50 based models on this linear probing (finetune last layer only) experiment.
                </p>

                <h3>
                    Learned representations exhibit interesting similarities, substantial differences
                </h3>
                <center><image src="imgs/teaser_star.png" height="240px"></image> </center> <br>
                <p class="text-justify">
                    In addition to the figure at the top of this page, we show uniformity and tolerance results. These suggest that, intuitively, methods with similar objectives, such as the clustering methods (DeepCluster, SwAV) might be more similar to each other than to other methods. However, this same line of reasoning would suggest these clustering-based methods are more alike to supervised learning than to the contrastive methods.
                </p>
                
                <h3>
                    Color-invariant training produces demonstrably color-invariant representations
                </h3>
                <center><image src="imgs/augmentation_invariance.png" height="240px"></image> </center> <br>
                <p class="text-justify">
                    We use CKA to compare representations of augmented and non-augmented images. We show that the unsupervised methods tend to produce very similar representations for augmentations which were used during their pre-training. 
                </p>

            </div>
        </div>

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <p class="text-justify">
                        To appear at CVPR 2022, Posters 2.2 (June 22, 2:30-5:00PM), poster 166b!    
                    </p>
                    <!--
                    <textarea id="bibtex" class="form-control" readonly rows="8">
                        @inproceedings{chen2021nerv, 
                        title={NeRV: Neural Representations for Videos}, 
                        author={Chen, Hao and He, Bo and Wang, Hanyu and Ren, Yixuan and Lim, Ser-Nam and Shrivastava, Abhinav}, 
                        booktitle={NeurIPS}, 
                        year={2021} 
                        }
                    </textarea>
                    -->
                 </div> 
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <br> The website template was borrowed from <a href="https://bmild.github.io">Ben Mildenhall</a> via <a href="https://haochen-rye.github.io/">Hao Chen</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>