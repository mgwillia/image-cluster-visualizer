<!DOCTYPE html>
<html>
<style>
    p {
        font-size: 15px;
    }
</style>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>10k Words</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1.">


    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <link rel="icon" type="image/png" href="img/seal_icon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110862391-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-110862391-1');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                A Video is Worth 10,000 Words: Training and Benchmarking with Diverse Captions for Better Long Video Retrieval<br />
                <small>
                    Submission Under Review
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://mgwillia.github.io">
                            Matt Gwilliam<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://mcogswell.io/">
                            Michael Cogswell<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=5tD6LNEAAAAJ&hl=en">
                            Meng Ye<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.ksikka.com/">
                            Karan Sikka<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="http://www.cs.umd.edu/~abhinav/">
                            Abhinav Shrivastava<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://sites.google.com/view/ajaydivakaran/home">
                            Ajay Divakaran<sup>2</sup>
                        </a>
                    </li>
                </ul>
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>University of Maryland, College Park 
                    </li>
                    <li>
                        <sup>2</sup>SRI International
                    </li>
                </ul>
                <!--University of Maryland, College Park -->
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2312.00115">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://drive.google.com/file/d/14S_E35Up6ANeRYU6uSEH_lBjUkx1BBuy/view?usp=sharing">
                            <h4><strong>Data</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Existing long video retrieval systems are trained and tested in the paragraph-to-video retrieval regime, 
                    where every long video is described by a single long paragraph. 
                    This neglects the richness and variety of 
                    possible valid descriptions of a video, which could be described in moment-by-moment detail, or in a single 
                    phrase summary, or anything in between. 
                    To provide a more thorough evaluation of the capabilities of long 
                    video retrieval systems, we propose a pipeline that leverages state-of-the-art large language models to 
                    carefully generate a diverse set of synthetic captions for long videos. 
                    We validate this pipeline's 
                    fidelity via rigorous human inspection. We then benchmark a representative set of video language models 
                    on these synthetic captions using a few long video datasets, showing that they struggle with the transformed 
                    data, especially the shortest captions. 
                    We also propose a lightweight fine-tuning method, where we use a 
                    contrastive loss to learn a hierarchical embedding loss based on the differing levels of information among 
                    the various captions. 
                    Our method improves performance both on the downstream paragraph-to-video retrieval 
                    task (+1.1% R@1 on ActivityNet), as well as for the various long video retrieval metrics we 
                    compute using our synthetic data (+3.6% R@1 for short descriptions on ActivityNet).
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <center>
                    <image src="imgs/10k_teaser.png" width="360px"></image>
                </center> <br>
                <p class="text-justify">
                    We find that existing long video retrieval datasets are arbitrarily constrained in terms of their text descriptions.
                    They use paragraph-length captions, which are often generated as highly literal descriptions of video content.
                    This neglects the richness of potential captions, especially shorter, abstract summaries.
                </p>

                <h3>
                    New Synthetic 10k Words Data
                </h3>
                <center>
                    <image src="imgs/10k_data.png" width="720px"></image>
                </center> <br>
                <p class="text-justify">
                    We develop a novel synthetic data generation pipeline, leveraging ChatGPT with GPT-3.5.
                    With our pipeline, we generate 10k Words supplements for 3 long video retrieval datasets: ActivityNet, QuerYD, and LF-VILA.
                    In the table above, we give an example of these synthetic captions.
                    You can download all 3 supplements from the link at the top of this page.
                </p>

                <h3>
                    SOTA Models Struggle with 10k Words Problem
                </h3>
                <center>
                    <image src="imgs/zero_shot_deltas.png" width="720px"></image>
                </center> <br>
                <p class="text-justify">
                    We find that in the zero-shot setting, a representative set of video-language models struggle with the 10k Words data for video retrieval.
                    They especially struggle with the shortest summaries.
                </p>

                <h3>
                    Novel Finetuning for 10k Words Problem
                </h3>
                <center>
                    <image src="imgs/10k_methods.png" width="720px"></image>
                </center> <br>
                <p class="text-justify">
                    We propose a novel finetuning strategy to improve performance for video retrieval, espeically the shorter summaries introduced by 10k.
                    At the core of this, we sample 10k Words data during training.
                    Additionally, we propose 2 losses to align a project of 10k text features with video and paragraph text features.
                </p>

                <h3>
                    Results
                </h3>
                <center>
                    <image src="imgs/10k_main_table.png" width="720px"></image>
                </center> <br>
                <p class="text-justify">
                    With our finetuning (applied to COSA), we achieve impressive improvements over zero-shot results compared to standard finetuning and standard finetuning with 10k Words data.
                </p>

            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly rows="8">
TBD
                    </textarea>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <br> The website was based on the popular template from <a href="https://bmild.github.io">Ben
                    Mildenhall</a>.
                </p>
            </div>
        </div>
    </div>
</body>

</html>